{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-20 17:39:24.435136: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import the relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import spacy\n",
    "import gensim.downloader as api\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction (feel free to change the title)\n",
    "\n",
    "The data pulled from the file titled *train_processed.csv* contains 4 columns - namely the *label* of each review, the review itself named *text*, *Tokens* obtained from the preprocessing of the text, and *Token string* which concatenates the tokens to form a sentence. The tokens only contain alphabetical words but do not contain stop words. Additionally, words that occur more than 7730 times and those that occur fewer than 6 times were removed from the list in an effort to reduce the complexity of the data to be handled by the methods below. \n",
    "\n",
    "The NLP methods implemented here are TFIDF, Doc2Vec, and Word2Vec, along with a combination of the three. The machine learning models used for classification are Naive Bayes, Logistic Regression, and Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Token string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Batch #5\\n\\nAppearance: Pours a slightly hazy ...</td>\n",
       "      <td>['batch', 'appearance', 'slightly', 'hazy', 'a...</td>\n",
       "      <td>batch appearance slightly hazy auburn color fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Murky peach color with off-white head. Aroma h...</td>\n",
       "      <td>['murky', 'peach', 'color', 'white', 'tart', '...</td>\n",
       "      <td>murky peach color white tart fruit kind minera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Can poured into a Spiegelau IPA glass\\n\\nA: Po...</td>\n",
       "      <td>['spiegelau', 'ipa', 'glass', 'golden', 'amber...</td>\n",
       "      <td>spiegelau ipa glass golden amber kinda creamy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>A big thanks to Jeff for this one. 750ml cappe...</td>\n",
       "      <td>['big', 'thank', 'jeff', 'cap', 'bottle', 'bro...</td>\n",
       "      <td>big thank jeff cap bottle brooklyn brewery sni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>On tap into a shaker pint.\\n\\nAppearance is go...</td>\n",
       "      <td>['tap', 'shaker', 'pint', 'appearance', 'golde...</td>\n",
       "      <td>tap shaker pint appearance golden amber lot la...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  \\\n",
       "0      0  Batch #5\\n\\nAppearance: Pours a slightly hazy ...   \n",
       "1      0  Murky peach color with off-white head. Aroma h...   \n",
       "2      0  Can poured into a Spiegelau IPA glass\\n\\nA: Po...   \n",
       "3      0  A big thanks to Jeff for this one. 750ml cappe...   \n",
       "4      0  On tap into a shaker pint.\\n\\nAppearance is go...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  ['batch', 'appearance', 'slightly', 'hazy', 'a...   \n",
       "1  ['murky', 'peach', 'color', 'white', 'tart', '...   \n",
       "2  ['spiegelau', 'ipa', 'glass', 'golden', 'amber...   \n",
       "3  ['big', 'thank', 'jeff', 'cap', 'bottle', 'bro...   \n",
       "4  ['tap', 'shaker', 'pint', 'appearance', 'golde...   \n",
       "\n",
       "                                        Token string  \n",
       "0  batch appearance slightly hazy auburn color fi...  \n",
       "1  murky peach color white tart fruit kind minera...  \n",
       "2  spiegelau ipa glass golden amber kinda creamy ...  \n",
       "3  big thank jeff cap bottle brooklyn brewery sni...  \n",
       "4  tap shaker pint appearance golden amber lot la...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the csv with the processed data\n",
    "df_train = pd.read_csv('train_processed.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the preprocessing, we can see that 37 reviews did not meet the criteria laid out above and were hence, removed from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21057 entries, 0 to 21056\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   label         21057 non-null  int64 \n",
      " 1   text          21057 non-null  object\n",
      " 2   Tokens        21057 non-null  object\n",
      " 3   Token string  21020 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 658.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Info of the processed data\n",
    "print(df_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21020 entries, 0 to 21019\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   label         21020 non-null  int64 \n",
      " 1   text          21020 non-null  object\n",
      " 2   Tokens        21020 non-null  object\n",
      " 3   Token string  21020 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 657.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Removing rows with NA values\n",
    "df_train.dropna(ignore_index=True, inplace=True)\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Cheng\n",
    "The 2 code cells below are to obtain the typos/words that aren't recognised by spacy. The counts of these unique words can be found in the files called *train_typos.csv* and *test_typos.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f760c206c5a4af88abb1c0389fec7cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21057 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count of typos in the train dataset\n",
    "typos_train = [[tkn.text for tkn in nlp(doc) if (tkn.is_oov) & (tkn.is_alpha)] for doc in tqdm(df_train.text)]\n",
    "\n",
    "train_typos_list = []\n",
    "for lst in typos_train:\n",
    "    for tkn in lst:\n",
    "        train_typos_list.append(tkn)\n",
    "\n",
    "print(len(train_typos_list))\n",
    "\n",
    "train_typo_count = Counter(train_typos_list)\n",
    "print(train_typo_count)\n",
    "\n",
    "df_train_typos = pd.DataFrame.from_dict(data=train_typo_count, orient='index', columns=['Count'])\n",
    "df_train_typos.to_csv('train_typos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b11bfd73524b5eb196b5b949967d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8943 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count of typos in the test dataset\n",
    "typos_test = [[tkn.text for tkn in nlp(doc) if (tkn.is_oov) & (tkn.is_alpha)] for doc in tqdm(df_test.text)]\n",
    "\n",
    "test_typos_list = []\n",
    "for lst in typos_test:\n",
    "    for tkn in lst:\n",
    "        test_typos_list.append(tkn)\n",
    "\n",
    "print(len(test_typos_list))\n",
    "\n",
    "test_typo_count = Counter(test_typos_list)\n",
    "print(test_typo_count)\n",
    "\n",
    "df_test_typos = pd.DataFrame.from_dict(data=test_typo_count, orient='index', columns=['Count'])\n",
    "df_test_typos.to_csv('test_typos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying list of pre-trained language models that can be used for the word2vec algorithm\n",
    "api.info()['models'].keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Not sure if this should come in the conclusion section. I'll leave that up to you\n",
    "\n",
    "The *word2vec-google-news-300* model was used as it is very comprehensive, however, we could look into using models from glove or fasttext in order to compare the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the appropriate pre-trained language model\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Word2Vec approach, we calculated the average value of the vector across a review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate average value of document vector\n",
    "def word_vectoriser(sent):\n",
    "    vector_size = wv.vector_size\n",
    "    wv_res = np.zeros(vector_size) # initialise vector\n",
    "\n",
    "    counter = 1\n",
    "    for word in sent:\n",
    "        if word in wv:\n",
    "            counter += 1 # increase counter\n",
    "            wv_res += wv[word] # sum of all word vectors within a review\n",
    "\n",
    "    wv_res = wv_res/counter # compute average value of vector\n",
    "    return wv_res # return average value of vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split data into train and validation data\n",
    "def split_data(X, y):\n",
    "    # applying train_test_split with the following parameters\n",
    "    # stratify is used to ensure all labels have the same density as the original dataset\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    return X_train, X_valid, y_train, y_valid # return split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run the naive bayes classifier\n",
    "def naive_bayes(X_train, X_valid, y_train, y_valid): \n",
    "    nb = MultinomialNB() # mulinomial naive bayes model is initialised\n",
    "    nb.fit(X_train, y_train) # data is fit to the model\n",
    "    y_pred_nb = nb.predict(X_valid) # prediction is made using validation data\n",
    "    print(f'Accuracy of Naive Bayes: {accuracy_score(y_valid, y_pred_nb)}') # accuracy of the model in classifying\n",
    "    print(f'Classification report:\\n{classification_report(y_valid, y_pred_nb)}') # summary of the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run the support vector classifier\n",
    "def svc(X_train, X_valid, y_train, y_valid):\n",
    "    svc = SVC(random_state=42) # initialised with random state of 42\n",
    "    svc.fit(X_train, y_train) # data is fit to the model\n",
    "    y_pred_svc = svc.predict(X_valid) # prediction is made using validation data\n",
    "    print(f'Accuracy of SVC: {accuracy_score(y_valid, y_pred_svc)}') # accuracy of the model in classifying\n",
    "    print(f'Classification report:\\n{classification_report(y_valid, y_pred_svc)}') # summary of the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run logistic regression\n",
    "def lr(X_train, X_valid, y_train, y_valid):\n",
    "    lr = LogisticRegression(random_state=42, solver='newton-cg') # initialised with random state of 42 and choice of solver\n",
    "    lr.fit(X_train, y_train) # data is fit to the model\n",
    "    y_pred_lr = lr.predict(X_valid) # prediction is made using validation data\n",
    "    print(f'Accuracy of Logistic Regression: {accuracy_score(y_valid, y_pred_lr)}') # accuracy of the model in classifying\n",
    "    print(f'Classification report:\\n{classification_report(y_valid, y_pred_lr)}') # summary of the classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### TFIDF\n",
    "\n",
    "The TFIDF vectoriser provided results of 58% accuracy by the Naive Bayes classifier, 60.27% accuracy by the Support Vector Classifier, and 59.77% accuracy by the Logistic Regression classifier. One notable observation is that the first 2 classifiers are not able to accurately classify label 1 accurately enough but is able to do a much better job with labels 0 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Bayes: 0.5801617507136061\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.67      0.64      1407\n",
      "           1       0.47      0.41      0.44      1393\n",
      "           2       0.64      0.66      0.65      1404\n",
      "\n",
      "    accuracy                           0.58      4204\n",
      "   macro avg       0.57      0.58      0.58      4204\n",
      "weighted avg       0.57      0.58      0.58      4204\n",
      "\n",
      "Accuracy of SVC: 0.6027592768791628\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.68      0.66      1407\n",
      "           1       0.49      0.43      0.46      1393\n",
      "           2       0.66      0.70      0.68      1404\n",
      "\n",
      "    accuracy                           0.60      4204\n",
      "   macro avg       0.60      0.60      0.60      4204\n",
      "weighted avg       0.60      0.60      0.60      4204\n",
      "\n",
      "Accuracy of Logistic Regression: 0.5977640342530923\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.68      0.65      1407\n",
      "           1       0.49      0.41      0.45      1393\n",
      "           2       0.65      0.70      0.68      1404\n",
      "\n",
      "    accuracy                           0.60      4204\n",
      "   macro avg       0.59      0.60      0.59      4204\n",
      "weighted avg       0.59      0.60      0.59      4204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer() # initialise the vectoriser\n",
    "tfidf_X_train, tfidf_X_valid, y_train, y_valid = split_data(df_train['Token string'], df_train.label) # split the data\n",
    "tfidf_train = vectorizer.fit_transform(tfidf_X_train) # fit the vectoriser to the train data\n",
    "tfidf_valid = vectorizer.transform(tfidf_X_valid) # vectorise the validation data using the transform method\n",
    "\n",
    "# running the various classifiers\n",
    "naive_bayes(tfidf_train, tfidf_valid, y_train, y_valid)\n",
    "svc(tfidf_train, tfidf_valid, y_train, y_valid)\n",
    "lr(tfidf_train, tfidf_valid, y_train, y_valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec\n",
    "\n",
    "The Doc2Vec approach requires us to build the vocabulary out of the tagged documents which are obtained from the training portion of the split dataset. We then infer the vectors for the train and validation datasets. The results are poorer compared to the previous approach with a 57.78% accuracy by the Support Vector Classifier, and 57.4% accuracy by the Logistic Regression Classifier. The Naive Bayes classifier cannot be run due to the vectors containing negative values which are not accepted by this classifier as it assumes a multinomial distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f41a6a03ac42ca946201f70a52acb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16816 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c7399fe43d4dd289e8c14c9654d4a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokens = [tkn.split() for tkn in df_train['Token string']] # splitting the token string to get the list of words in each review\n",
    "doc2vec_X_train, doc2vec_X_valid, y_train, y_valid = split_data(tokens, df_train.label) # split the data\n",
    "\n",
    "# the section below is for the doc2vec algorithm\n",
    "X_tagged_docs_train = [TaggedDocument(doc, [i]) for i, doc in enumerate(doc2vec_X_train)] # create a list of tagged documents\n",
    "model = Doc2Vec(X_tagged_docs_train, vector_size=100, window=2, min_count=10, workers=4, epochs=30) # initialising the doc2vec model\n",
    "model.build_vocab(X_tagged_docs_train) # building the vocabulary based on the train data\n",
    "model.train(X_tagged_docs_train, total_examples=model.corpus_count, epochs=model.epochs) # training the model\n",
    "doc2vec_X_train = [model.infer_vector(doc.words) for doc in tqdm(X_tagged_docs_train)] # inferring vectors in the train dataset\n",
    "\n",
    "X_tagged_docs_valid = [TaggedDocument(doc, [i]) for i, doc in enumerate(doc2vec_X_valid)] # create a list of tagged documents\n",
    "# inferring the vectors for the validation based on the model that was trained on the train dataset\n",
    "doc2vec_X_valid = [model.infer_vector(doc.words) for doc in tqdm(X_tagged_docs_valid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVC: 0.5777830637488106\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.64      0.63      1407\n",
      "           1       0.47      0.37      0.42      1393\n",
      "           2       0.61      0.72      0.66      1404\n",
      "\n",
      "    accuracy                           0.58      4204\n",
      "   macro avg       0.57      0.58      0.57      4204\n",
      "weighted avg       0.57      0.58      0.57      4204\n",
      "\n",
      "Accuracy of Logistic Regression: 0.5739771646051379\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.66      0.63      1407\n",
      "           1       0.47      0.29      0.36      1393\n",
      "           2       0.59      0.78      0.67      1404\n",
      "\n",
      "    accuracy                           0.57      4204\n",
      "   macro avg       0.56      0.57      0.55      4204\n",
      "weighted avg       0.56      0.57      0.55      4204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# running the various classifiers\n",
    "svc(doc2vec_X_train, doc2vec_X_valid, y_train, y_valid)\n",
    "lr(doc2vec_X_train, doc2vec_X_valid, y_train, y_valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Doc2Vec and TFIDF\n",
    "\n",
    "Here, we combine the output vectors obtained from the above 2 approaches and stack them in the hopes of obtaining an improvement in the accuracy of classification\n",
    "\n",
    "We notice that the results don't improve considerably with the accuracy of the Support Vector Classifier being 58.25% and the accuracy of the Logistics Regression Classifier being 60.1%. This is probably because the vectors obtained don't contain additional information that is helpful for the classification. Notice again that both classifiers are not able to accurately classify label 1 as it is for labels 0 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "# merging the train vectors obtained from the doc2vec and TFIDF algorithms\n",
    "merged_features_train = hstack((tfidf_train, doc2vec_X_train))\n",
    "\n",
    "# merging the validation vectors obtained from the doc2vec and TFIDF algorithms\n",
    "merged_features_valid = hstack((tfidf_valid, doc2vec_X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVC: 0.5825404376784015\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.65      0.64      1407\n",
      "           1       0.47      0.38      0.42      1393\n",
      "           2       0.62      0.72      0.67      1404\n",
      "\n",
      "    accuracy                           0.58      4204\n",
      "   macro avg       0.57      0.58      0.57      4204\n",
      "weighted avg       0.57      0.58      0.57      4204\n",
      "\n",
      "Accuracy of Logistic Regression: 0.6008563273073264\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.69      0.66      1407\n",
      "           1       0.49      0.39      0.43      1393\n",
      "           2       0.65      0.71      0.68      1404\n",
      "\n",
      "    accuracy                           0.60      4204\n",
      "   macro avg       0.59      0.60      0.59      4204\n",
      "weighted avg       0.59      0.60      0.59      4204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# running the various classifiers\n",
    "svc(merged_features_train, merged_features_valid, y_train, y_valid)\n",
    "lr(merged_features_train, merged_features_valid, y_train, y_valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Doc2Vec, TFIDF, and Word2Vec\n",
    "\n",
    "Here, we combine the output vectors obtained from all 3 approaches and compare the results. We notice once again that the results haven't improved indicating that no new information is being contained in the vectors generated to be helpful for classification. The accuracy of the Support Vector Classifier is at 58% and that for the Logistics Regression Classifier is at 60.1%. Notice once again that both classifiers are not able to accurately classify label 1 as it is for labels 0 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data for the word2vec approach\n",
    "word2vec_X_train, word2vec_X_valid, y_train, y_valid = split_data(tokens, df_train.label)\n",
    "word2vec_train = []\n",
    "for doc_tkn in word2vec_X_train:\n",
    "    try:\n",
    "        word2vec_train.append(word_vectoriser(doc_tkn)) # calling the word_vectoriser function to vectorise each review\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "word2vec_valid = []\n",
    "for doc_tkn in word2vec_X_valid:\n",
    "    try:\n",
    "        word2vec_valid.append(word_vectoriser(doc_tkn)) # calling the word_vectoriser function to vectorise each review\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# combining the output vectors from all three approaches\n",
    "combined_embeddings_train = hstack((word2vec_train, merged_features_train))\n",
    "combined_embeddings_valid = hstack((word2vec_valid, merged_features_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVC: 0.5799238820171265\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.66      0.63      1407\n",
      "           1       0.46      0.36      0.41      1393\n",
      "           2       0.63      0.72      0.67      1404\n",
      "\n",
      "    accuracy                           0.58      4204\n",
      "   macro avg       0.57      0.58      0.57      4204\n",
      "weighted avg       0.57      0.58      0.57      4204\n",
      "\n",
      "Accuracy of Logistic Regression: 0.6008563273073264\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.69      0.66      1407\n",
      "           1       0.49      0.39      0.43      1393\n",
      "           2       0.66      0.72      0.69      1404\n",
      "\n",
      "    accuracy                           0.60      4204\n",
      "   macro avg       0.59      0.60      0.59      4204\n",
      "weighted avg       0.59      0.60      0.59      4204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# running the various classifiers\n",
    "svc(combined_embeddings_train, combined_embeddings_valid, y_train, y_valid)\n",
    "lr(combined_embeddings_train, combined_embeddings_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coursework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
